# Inference Server Config for GridGame
# Configures vLLM inference engine for vision-language model

[model]
# MUST match the model in train.toml and orch.toml
name = "Qwen/Qwen2.5-VL-3B-Instruct"

# Data type for inference
dtype = "auto"  # Use auto for optimal precision

# Maximum model context length
max_model_len = 8192

# Trust remote code for VLMs
trust_remote_code = true

# Disable eager mode for better performance (use CUDA graphs)
enforce_eager = false

[parallel]
# Parallelism settings for inference
tp = 1  # Tensor parallelism (set to GPU count if using multiple GPUs)
dp = 1  # Data parallelism for higher throughput

